{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Access and train remotely in Azure Machine Learning\n",
    "\n",
    "The data in this demo comes from the [Chicago Parking Ticket database, courtesy of Daniel Hutmacher](https://sqlsunday.com/2022/12/05/new-demo-database/).  I sampled 1,000,000 records from it and [the file I used is available in CSV format](https://cspolybasepublic.blob.core.windows.net/cstrainingpublicdata/ChicagoParkingTickets.txt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create handle to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "SUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\") \n",
    "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\")\n",
    "AML_WORKSPACE_NAME = os.getenv(\"AML_WORKSPACE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675966726847
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=AML_WORKSPACE_NAME,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "## Upload data to cloud storage\n",
    "\n",
    "* Create a new Dataset of type Tabular and upload file located here:  \n",
    "../../data/ChicagoParkingTickets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675461156382
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# use the dataset in Azure ML Studio\n",
    "data_asset_name = \"ChicagoFares\"\n",
    "v1 = \"1\"\n",
    "\n",
    "try:\n",
    "    data_asset = ml_client.data.get(name=data_asset_name, version=v1)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {data_asset_name}, version: {v1}\"\n",
    "    )\n",
    "except:\n",
    "    print(f\"Data asset not found. Upload file ../../data/ChicagoParkingTickets.txt to Azure ML and create a dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name.\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input\n",
    "\n",
    "# define the command\n",
    "command_job = command(\n",
    "    code=\"./src\",\n",
    "    command=\"python main.py --iris-csv ${{inputs.iris_csv}} --learning-rate ${{inputs.learning_rate}} --boosting ${{inputs.boosting}}\",\n",
    "    environment=\"AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu@latest\",\n",
    "    inputs={\n",
    "        \"iris_csv\": Input(\n",
    "            type=\"uri_file\",\n",
    "            path=\"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\",\n",
    "        ),\n",
    "        \"learning_rate\": 0.9,\n",
    "        \"boosting\": \"gbdt\",\n",
    "    },\n",
    "    compute=\"cpu-cluster\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675445030495
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get a handle of the data asset and print the URI\n",
    "data_asset = ml_client.data.get(name=data_asset_name, version=v1)\n",
    "print(f\"Data asset URI: {data_asset.path}\")\n",
    "\n",
    "# read into pandas - note that you will see 2 headers in your data frame - that is ok, for now\n",
    "\n",
    "df = pd.read_csv(data_asset.path)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create a new version of the data asset\n",
    "\n",
    "You might have noticed that the data needs a little light cleaning, to make it fit to train a machine learning model. It has:\n",
    "\n",
    "* two headers\n",
    "* a client ID column; we wouldn't use this feature in Machine Learning\n",
    "* spaces in the response variable name\n",
    "\n",
    "Also, compared to the CSV format, the Parquet file format becomes a better way to store this data. Parquet offers compression, and it maintains schema. Therefore, to clean the data and store it in Parquet, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675445038545
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# read in data again, this time using the 2nd row as the header\n",
    "df = pd.read_csv(data_asset.path, header=1)\n",
    "# rename column\n",
    "df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "# remove ID column\n",
    "df.drop(\"ID\", axis=1, inplace=True)\n",
    "\n",
    "# write file to filesystem\n",
    "df.to_parquet(\"./data/cleaned-credit-card.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "This table shows the structure of the data in the original **default_of_credit_card_clients.csv** file .CSV file downloaded in an earlier step. The uploaded data contains 23 explanatory variables and 1 response variable, as shown here:\n",
    "\n",
    "|Column Name(s) | Variable Type  |Description  |\n",
    "|---------|---------|---------|\n",
    "|X1     |   Explanatory      |    Amount of the given credit (NT dollar): it includes both the individual consumer credit and their family (supplementary) credit.    |\n",
    "|X2     |   Explanatory      |   Gender (1 = male; 2 = female).      |\n",
    "|X3     |   Explanatory      |   Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).      |\n",
    "|X4     |   Explanatory      |    Marital status (1 = married; 2 = single; 3 = others).     |\n",
    "|X5     |   Explanatory      |    Age (years).     |\n",
    "|X6-X11     | Explanatory        |  History of past payment. We tracked the past monthly payment records (from April to September  2005). -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.      |\n",
    "|X12-17     | Explanatory        |  Amount of bill statement (NT dollar) from April to September  2005.      |\n",
    "|X18-23     | Explanatory        |  Amount of previous payment (NT dollar) from April to September  2005.      |\n",
    "|Y     | Response        |    Default payment (Yes = 1, No = 0)     |\n",
    "\n",
    "Next, create a new _version_ of the data asset (the data automatically uploads to cloud storage).  For this version, we'll add a time value, so that each time this code is run, a different version number will be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675382989789
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "import time\n",
    "\n",
    "# Next, create a new *version* of the data asset (the data is automatically uploaded to cloud storage):\n",
    "# v2 = \"cleaned\" + time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
    "v2 = \"cleaned\"\n",
    "\n",
    "my_path = \"./data/cleaned-credit-card.parquet\"\n",
    "\n",
    "# Define the data asset, and use tags to make it clear the asset can be used in training\n",
    "\n",
    "my_data = Data(\n",
    "    name=data_asset_name,\n",
    "    version=v2,\n",
    "    description=\"Default of credit card clients data.\",\n",
    "    tags={\"training_data\": \"true\", \"format\": \"parquet\"},\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "## create the data asset\n",
    "\n",
    "my_data = ml_client.data.create_or_update(my_data)\n",
    "\n",
    "print(f\"Data asset created. Name: {my_data.name}, version: {my_data.version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The cleaned parquet file is the latest version data source. This code shows the CSV version result set first, then the Parquet version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1675383001940
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get a handle of the data asset and print the URI\n",
    "data_asset_v1 = ml_client.data.get(name=data_asset_name, version=v1)\n",
    "data_asset_v2 = ml_client.data.get(name=data_asset_name, version=v2)\n",
    "\n",
    "# print the v1 data\n",
    "print(f\"V1 Data asset URI: {data_asset_v1.path}\")\n",
    "v1df = pd.read_csv(data_asset_v1.path)\n",
    "print(v1df.head(5))\n",
    "\n",
    "# print the v2 data\n",
    "print(\n",
    "    \"_____________________________________________________________________________________________________________\\n\"\n",
    ")\n",
    "print(f\"V2 Data asset URI: {data_asset_v2.path}\")\n",
    "v2df = pd.read_parquet(data_asset_v2.path)\n",
    "print(v2df.head(5))"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Upload data to cloud storage, create a data asset, create new versions for data assets, use the data for interactive development."
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
